# At the Intersection of LLMs and Kernels - Research Roundup

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png)

## Metadata
- Author: [[charlesfrye.github.io]]
- Full Title: At the Intersection of LLMs and Kernels - Research Roundup
- Category: #articles
- Summary: Large language models (LLMs) are a new building block in machine learning research. This post provides explainers for papers that apply systems metaphors to improve LLMs, such as speculative execution, registers, paged memory, and virtual memory. Speculative execution involves executing cheap but potentially wrong operations to speed up LLM inference. Registers are used in Vision Transformers to store intermediate information. Paged memory improves memory management in LLMs by allocating memory in multiples of moderately-sized blocks. Virtual memory allows LLMs to effectively access much larger storage. These improvements aim to enhance the performance and capabilities of LLMs.
- URL: https://charlesfrye.github.io/programming/2023/11/10/llms-systems.html

## Highlights
- The intuition behind speculative sampling is that some of the tokens in the output are very easy to guess – a period will come at the end of this sentence, for example – and so can be predicted very cheaply.
  These methods further take advantage of the fact that computing the logprobs for a prompt + K tokens can be done in parallel. That makes it much cheaper than sampling K tokens to follow a prompt, which must be done serially – a painful and unavoidable fact of autoregressive models like Transformers or RNNs.
  The K tokens following the prompt are generated by some cheap strategy – a smaller language model or even a heuristic rule. We then compute the logprobs for the prompt + K tokens in the large model, in parallel, and then throw out only those tokens that are improbable according to the large model. ([View Highlight](https://read.readwise.io/read/01hqkx1mzqbrc1hqzgaehmx3sf))
- Because each layer interacts additively with its inputs – the output of a layer with input `x` can be written as `f(x) + x` – Transformer layers can treat their inputs like a content-addressable memory, reading information written by earlier layers and writing in some of their own. For more on this view of Transformer internals, see the excellent [work from Anthropic’s interpretability team](https://transformer-circuits.pub).
  It’s a neat way to do computation, but it has a big issue: if you want to write information that’s not tied to a specific token position, you don’t have a good place to put it.
  It is hypothesized in the paper that large ViTs learn to use the least-informative token areas as a kind of scratchpad – like stealing a block of memory, using it to store intermediate values, and hoping you don’t break anything.
  The paper proposes adding some blank “register” tokens to the input in order to give the model a safe spot to store the results of calculations that don’t otherwise have a place to go.
  The addition of registers does not meaningfully improve performance on benchmarks (cf. Table 2 in [the paper](https://arxiv.org/abs/2309.16588)), but the resulting attention maps are much cleaner, as is obvious from the figure above, and so one might expect some downstream benefits, e.g. to interpretability or steerability. ([View Highlight](https://read.readwise.io/read/01hqkx39qzg80xxc2s9t37qb2h))
- Memory must be allocated, and those allocations must be tracked and managed, e.g. defragmented, to maximize utilization of memory resources, which were precious even before [the Chrome Nation attacked](https://knowyourmeme.com/memes/google-chrome-ram-hog).
  Memory allocators must handle a gnar trade-off: address-wise allocations make efficient use of memory in the best case, but lead to fragmentation and expensive allocator operations in reality, while large per-process/thread allocations are easy to manage but lead to wasted memory in the generic case that the allocations are not fully utilized. This particular form of under-utilization is called “internal fragmentation”.
  The solution is to split the difference and allocate memory in multiples of moderately-sized blocks called “pages”. ([View Highlight](https://read.readwise.io/read/01hqkx40s0qyh8bb3p69be3mcp))
- The focus of memory management in LLM inference is on the key-value (“KV”) cache, used to store the intermediate values of attention computations, for re-use when computing the outputs for future tokens in the sequence. This cache, introduced by the 2022 paper [Efficiently Scaling Tansformer Inference](https://arxiv.org/abs/2211.05102) from Google, converts a quadratic-time operation (compute attention for all token pairs in the sequence) into a linear-time one (compute only attention for the newest token in the sequence), at a linear cost in space (store keys and values for each token in the sequence). ([View Highlight](https://read.readwise.io/read/01hqkx5c3yqhz66p054vvb2ewj))
- As in retrieval-augmented generation (RAG) patterns, the addressable information is stored outside the LLM and interfaced with by traditional software. That means it can be a database, a filesystem, or any other form of storage.
  But unlike typical RAG, in MemGPT retrieval from the external context is done by the LLM, via function-calling tool use, rather than being hard-coded. This style is associated more with agent patterns and is [used in the OpenAI Assistants API](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval).
  The LLM is prompted to retrieve the information it needs and then add it to its prompt – much like virtual memory stored on disk can be paged into RAM.
  Even further down the path to agency and away from simple RAG, in the MemGPT pattern the LLM is also responsible for writing information that overflows the context window back to the storage system. This pattern was popularized by the [Generative Agents paper](https://arxiv.org/abs/2304.03442), which uses LLMs to drive video game NPCs. There, agents had the ability to “reflect” on their experiences and write those reflections into their memory. ([View Highlight](https://read.readwise.io/read/01hqkx8c7rcmtrb459e5j650mh))
- A key novelty of the MemGPT approach, relative to other similar systems, is the event-driven style: the LLM sits idle, waiting for events like user messages, timer ticks, or state changes in the world.
  This is an extremely common pattern in the systems discipline – on every keystroke, button press, or filesystem change, a typical operating system interrupts running processes to respond to the event, e.g. to pass the keystroke to the process managing the active window. [Browsers are similarly event-driven](https://www.youtube.com/watch?v=8aGhZQkoFbQ).
  Interruptibility and event-driven-ness are key features of biological agents as well. To quote Copilot’s suggested completion of this paragraph: “If you poke a cat, it will stop what it’s doing and respond to the poke.”
  When that suggestion appeared (an event) it stopped me from writing (interrupted my writing process). I was triggered to reflect on it (I handled the interrupt by updating my state). I then went back to writing (resumed my writing process) with a new idea for what to say next.
  It is clear that if LLMs are to become the cognitive kernels of agents in any way worthy of the name, they will need a similar architecture.
  So while drawing architectures [from the computational cognitive science literature](https://arxiv.org/abs/2309.02427) is a good idea, we as MLRs should take care to draw from the work on engineering operating systems, browsers, database engines, and other systems as well. ([View Highlight](https://read.readwise.io/read/01hqkx9t3gvb15n2t7zqcrgtnz))
